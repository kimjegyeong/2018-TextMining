{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "# Text Mining 2018\n",
    "\n",
    "## Assignment 2\n",
    "</center><div class=\"pull-right\">\n",
    "김제경 20176005 \n",
    "이희랑 20186014  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of assignment2 is to derive and implement PLSA(Probabilistic Latent Semantic Analysis) algorithm.  \n",
    "In this assignment, we will compare two kinds of approachs;  \n",
    "** 1) One is the model without Background Language Model, **  \n",
    "** 2) The other is the model with Background Language Model **\n",
    "\n",
    "The reason why we implement two approach is we want to compare the result and performance of two methods, which are to remove stopwords directly in preprocessing step and to define stopwords by using background language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing for documents is same as previous assignment, so we put preprocessing code in the end of the report.   \n",
    "Now we have only 10 documents, but we plan to gather more data for our term project purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'E:\\\\unist\\\\2-1\\\\abstracts2.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d82c998866b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'E:\\\\unist\\\\2-1\\\\'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'abstracts2.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cp949'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'E:\\\\unist\\\\2-1\\\\abstracts2.csv' does not exist"
     ]
    }
   ],
   "source": [
    "path = 'E:\\\\unist\\\\2-1\\\\'\n",
    "df = pd.read_csv(path + 'abstract_preprocessed.csv', encoding='cp949')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  \n",
    "### 1.1. Make TDM using sklearn package\n",
    "we defined 2 corpus with stopwords and without stopwords to compare performance of Background Language Model(probablistic stopwords define) with method which is to remove stopwords directly in preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus without stopwords\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "corpus1 = vect.fit_transform(df['all'].tolist()).toarray()\n",
    "\n",
    "# corpus with stopwords\n",
    "vect2 = CountVectorizer(stop_words=None, min_df=0, max_df=100000)\n",
    "corpus2 = vect2.fit_transform(df['all'].tolist()).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 484)\n",
      "(10, 577)\n"
     ]
    }
   ],
   "source": [
    "print(corpus1.shape)\n",
    "print(corpus2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 2 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** small additional preprocessing **   \n",
    "We removed the words appeared only once because we consider those\n",
    "are meaningless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 196)\n",
      "(10, 249)\n"
     ]
    }
   ],
   "source": [
    "corpus_noback = corpus1[:, np.sum(corpus1, axis=0)>1]\n",
    "corpus_back = corpus2[:, np.sum(corpus2, axis=0)>1]\n",
    "\n",
    "print(corpus_noback.shape)\n",
    "print(corpus_back.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_noback = [a for a,b in zip(sorted(list(vect.vocabulary_.keys())), np.sum(corpus1, axis=0)>1) if b ]\n",
    "words_back = [a for a,b in zip(sorted(list(vect2.vocabulary_.keys())), np.sum(corpus2, axis=0)>1) if b ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10221205186880244"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_index = [i for i, a in enumerate(words_back) if a not in words_noback]\n",
    "lamb = np.sum([a for i, a in enumerate(corpus_back) if i in stopwords_index]) / np.sum(corpus_back)\n",
    "lamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> By comparing shape of corpus in above, we can assume that there are 56 stopwords in corpus.  \n",
    "-> In this manner, we will set λb = (total count of stopwords / total count of words) ~ 0.10 later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "## 2. PLSA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for compute normalize\n",
    "def normalize(vec):\n",
    "    s = sum(vec)\n",
    "    for i in range(len(vec)):\n",
    "        vec[i] = vec[i] * 1.0 / s   \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PLSA(tdm, number_of_documents, number_of_vocabs, number_of_topics, lamb, number_of_iteration):\n",
    "    \n",
    "    # Initialize parameter by random\n",
    "    pi = normalize(np.random.random(size = (number_of_documents, number_of_topics)))\n",
    "    topic_word_prob = normalize(np.random.random(size = (number_of_topics, number_of_vocabs)))  \n",
    "    background_word_prob = np.sum(tdm, axis=0) / np.sum(tdm)\n",
    "\n",
    "    # Initialize topic probability and background probability \n",
    "    topic_prob = np.zeros([number_of_documents, number_of_vocabs, number_of_topics], dtype=np.float)\n",
    "    background_prob = np.zeros([number_of_documents, number_of_vocabs], dtype=np.float)\n",
    "\n",
    "    iteration = number_of_iteration\n",
    "    ll = []\n",
    "    for i in range(iteration):\n",
    "\n",
    "        print('%i\\'s Iteration' % (i))\n",
    "        print('E-step')\n",
    "        for d_i, document in enumerate(tdm):\n",
    "            for w_i in range(number_of_vocabs):\n",
    "                prob = pi[d_i, :] * topic_word_prob[:, w_i]\n",
    "                topic_prob[d_i][w_i] = normalize(prob)\n",
    "\n",
    "                background_prob[d_i][w_i] = (lamb * background_word_prob[w_i]) / ((lamb * background_word_prob[w_i]) + ((1-lamb) * np.sum(pi[d_i, :] * topic_word_prob[:, w_i])))\n",
    "\n",
    "        print('M-step')\n",
    "        for d_i in range(number_of_documents):\n",
    "            for j in range(number_of_topics):\n",
    "                s = 0\n",
    "                for w_i in range(number_of_vocabs):\n",
    "                    count = tdm[d_i][w_i]\n",
    "                    s += count * (1-background_prob[d_i][w_i]) * topic_prob[d_i, w_i, j]\n",
    "                pi[d_i][j] = s\n",
    "            pi[d_i] = normalize(pi[d_i])\n",
    "\n",
    "        for j in range(number_of_topics):\n",
    "            for w_i in range(number_of_vocabs):\n",
    "                s = 0\n",
    "                for d_i in range(number_of_documents):\n",
    "                    count = tdm[d_i][w_i]\n",
    "                    s += count * (1-background_prob[d_i][w_i]) * topic_prob[d_i, w_i, j]\n",
    "                topic_word_prob[j][w_i] = s\n",
    "            topic_word_prob[j] = normalize(topic_word_prob[j])\n",
    "\n",
    "        # log-likelihood\n",
    "        print('Calculate Log-Likelihood')\n",
    "        likelihood = 0\n",
    "        for d_i in range(number_of_documents):\n",
    "            for w_i in range(number_of_vocabs):\n",
    "                likelihood += tdm[d_i][w_i] * np.log((lamb * background_word_prob[w_i]) + ((1-lamb) *  np.sum(pi[d_i, :] * topic_word_prob[:, w_i])))\n",
    "        print(likelihood, '\\n')\n",
    "        ll.append(likelihood)\n",
    "    \n",
    "    return pi, topic_word_prob, ll, background_word_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \n",
    "### 2.1. Compute Log-Likelihood in 2 ways\n",
    "#### 1. Model without Background Language Model\n",
    " - Using corpus without stopwords (preprocessed by nltk's stopwords)\n",
    " - As don't use backgoround language model, set λb = 0\n",
    " \n",
    "#### 2. Model applying Background Language Model\n",
    " - Using corpus with stopwords\n",
    " - λb = (total count of stopwords / total count of words) ~ 0.1\n",
    " \n",
    "We chose the number of topics to 2, because we gather our data in two category; NLP, Robotics\n",
    "  \n",
    "-------------------------------------------------------------------------------- \n",
    "  \n",
    "*** We set random seed to '1' because we need to compare the PLSA result with same initialization point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "2. 전체 단어를 포함한 Corpus 사용(Background Language Model)\n",
      "0's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6477.11031526 \n",
      "\n",
      "1's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6443.16364739 \n",
      "\n",
      "2's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6407.8318217 \n",
      "\n",
      "3's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6371.41476444 \n",
      "\n",
      "4's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6340.41847902 \n",
      "\n",
      "5's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6317.54595603 \n",
      "\n",
      "6's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6301.56175834 \n",
      "\n",
      "7's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6290.68261507 \n",
      "\n",
      "8's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6283.00872049 \n",
      "\n",
      "9's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6277.332939 \n",
      "\n",
      "10's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6273.2685342 \n",
      "\n",
      "11's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6270.51726236 \n",
      "\n",
      "12's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6268.62605979 \n",
      "\n",
      "13's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6267.20383043 \n",
      "\n",
      "14's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6265.98556548 \n",
      "\n",
      "15's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6264.82561075 \n",
      "\n",
      "16's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6263.69936953 \n",
      "\n",
      "17's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6262.65051377 \n",
      "\n",
      "18's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6261.70464053 \n",
      "\n",
      "19's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6260.86681544 \n",
      "\n",
      "20's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6260.13982176 \n",
      "\n",
      "21's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6259.51716137 \n",
      "\n",
      "22's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6258.97987258 \n",
      "\n",
      "23's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6258.5019423 \n",
      "\n",
      "24's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6258.05708824 \n",
      "\n",
      "25's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6257.62244745 \n",
      "\n",
      "26's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6257.17984286 \n",
      "\n",
      "27's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6256.7172103 \n",
      "\n",
      "28's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6256.23084784 \n",
      "\n",
      "29's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6255.73039406 \n",
      "\n",
      "30's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6255.24746314 \n",
      "\n",
      "31's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6254.82565339 \n",
      "\n",
      "32's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6254.48356283 \n",
      "\n",
      "33's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6254.20382981 \n",
      "\n",
      "34's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6253.95458261 \n",
      "\n",
      "35's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6253.7036369 \n",
      "\n",
      "36's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6253.42042023 \n",
      "\n",
      "37's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6253.06395938 \n",
      "\n",
      "38's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6252.59229607 \n",
      "\n",
      "39's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6252.06857615 \n",
      "\n",
      "40's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6251.65269031 \n",
      "\n",
      "41's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6251.3709046 \n",
      "\n",
      "42's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6251.15942415 \n",
      "\n",
      "43's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6250.98654751 \n",
      "\n",
      "44's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6250.84390837 \n",
      "\n",
      "45's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6250.72932174 \n",
      "\n",
      "46's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6250.64021024 \n",
      "\n",
      "47's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6250.57194872 \n",
      "\n",
      "48's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6250.51844184 \n",
      "\n",
      "49's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6250.4705985 \n",
      "\n",
      "50's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6250.40553965 \n",
      "\n",
      "51's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6250.25716482 \n",
      "\n",
      "52's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.95514422 \n",
      "\n",
      "53's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.6880837 \n",
      "\n",
      "54's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.59308094 \n",
      "\n",
      "55's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.54707216 \n",
      "\n",
      "56's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.50375009 \n",
      "\n",
      "57's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.46244203 \n",
      "\n",
      "58's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.42866673 \n",
      "\n",
      "59's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.40505315 \n",
      "\n",
      "60's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.38984695 \n",
      "\n",
      "61's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.37989003 \n",
      "\n",
      "62's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.37289576 \n",
      "\n",
      "63's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.36766157 \n",
      "\n",
      "64's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.36360035 \n",
      "\n",
      "65's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.36039619 \n",
      "\n",
      "66's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.35784969 \n",
      "\n",
      "67's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.35581882 \n",
      "\n",
      "68's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.35419597 \n",
      "\n",
      "69's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.3528974 \n",
      "\n",
      "70's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.35185719 \n",
      "\n",
      "71's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.35102309 \n",
      "\n",
      "72's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.35035353 \n",
      "\n",
      "73's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34981528 \n",
      "\n",
      "74's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34938162 \n",
      "\n",
      "75's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34903084 \n",
      "\n",
      "76's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34874496 \n",
      "\n",
      "77's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34850853 \n",
      "\n",
      "78's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34830748 \n",
      "\n",
      "79's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.3481277 \n",
      "\n",
      "80's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34795313 \n",
      "\n",
      "81's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.3477631 \n",
      "\n",
      "82's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34752806 \n",
      "\n",
      "83's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34720299 \n",
      "\n",
      "84's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.3467167 \n",
      "\n",
      "85's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34595515 \n",
      "\n",
      "86's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34473578 \n",
      "\n",
      "87's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.34277077 \n",
      "\n",
      "88's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.33962068 \n",
      "\n",
      "89's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.33465331 \n",
      "\n",
      "90's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.32705391 \n",
      "\n",
      "91's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.31598382 \n",
      "\n",
      "92's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.30100825 \n",
      "\n",
      "93's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.28275579 \n",
      "\n",
      "94's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.26331188 \n",
      "\n",
      "95's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.24556882 \n",
      "\n",
      "96's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.2316558 \n",
      "\n",
      "97's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.2219367 \n",
      "\n",
      "98's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.21547449 \n",
      "\n",
      "99's Iteration\n",
      "E-step\n",
      "M-step\n",
      "Calculate Log-Likelihood\n",
      "-6249.21107765 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "iteration = 100\n",
    "'''\n",
    "print('1. Stopword를 임의로 제거(패키지의 stopword 이용)해준 Corpus 사용 : λb = 0')\n",
    "np.random.seed(1)\n",
    "pi_noback, topic_word_prob_noback, likelihood_noback = PLSA(corpus_noback, corpus_noback.shape[0], corpus_noback.shape[1], 2, 0, iteration)\n",
    "'''\n",
    "\n",
    "print('\\n\\n\\n2. 전체 단어를 포함한 Corpus 사용(Background Language Model)')\n",
    "np.random.seed(1)\n",
    "pi_back, topic_word_prob_back, likelihood_back, back = PLSA(corpus_back, corpus_back.shape[0], corpus_back.shape[1], 2, lamb, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Word Distribution Sorted by Topic 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.04958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.04195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.03814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.02822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.02517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.02441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.02365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0.02288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.01831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robot</th>\n",
       "      <td>0.01678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reinforcement</th>\n",
       "      <td>0.01526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <td>0.01297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>network</th>\n",
       "      <td>0.01297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>0.01297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>policy</th>\n",
       "      <td>0.01220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>0.01144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>0.01068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>0.01068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.00992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>0.00915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Topic1\n",
       "the            0.04958\n",
       "to             0.04195\n",
       "of             0.03814\n",
       "and            0.02822\n",
       "we             0.02517\n",
       "in             0.02441\n",
       "learning       0.02365\n",
       "that           0.02288\n",
       "model          0.01831\n",
       "robot          0.01678\n",
       "reinforcement  0.01526\n",
       "task           0.01297\n",
       "network        0.01297\n",
       "on             0.01297\n",
       "policy         0.01220\n",
       "for            0.01144\n",
       "with           0.01068\n",
       "training       0.01068\n",
       "this           0.00992\n",
       "neural         0.00915"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nTopic Word Distribution Sorted by Topic 1')\n",
    "pd.DataFrame(np.round(back.T, 5), columns = ['Topic1'],\\\n",
    "            index = words_back).sort_values('Topic1', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood of the PLSA without background language model is -3537.5972008\n",
      "Log-likelihood of the PLSA with background language model is -6249.21107765\n"
     ]
    }
   ],
   "source": [
    "print('Log-likelihood of the PLSA without background language model is', likelihood_noback[-1])\n",
    "print('Log-likelihood of the PLSA with background language model is', likelihood_back[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-likelihood of the PLSA without background language model is smaller than the other.  \n",
    "So, we will use PLSA with background language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 iteration's log-likelihood : [-3794.7740197260196, -3755.7057788517482, -3713.1587442420996]\n",
      "Last 3 iteration's log-likelihood : [-3537.5972202549028, -3537.5972110530761, -3537.5972007963865]\n"
     ]
    }
   ],
   "source": [
    "print('First 3 iteration\\'s log-likelihood :', likelihood_noback[:3])\n",
    "print('Last 3 iteration\\'s log-likelihood :', likelihood_noback[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "## 3. Conclusion\n",
    "### 3.1. Check P(z|d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc 1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 2</th>\n",
       "      <td>0.44050</td>\n",
       "      <td>0.55950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 3</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 4</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 5</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 6</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 7</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 8</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 9</th>\n",
       "      <td>0.94327</td>\n",
       "      <td>0.05673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc10</th>\n",
       "      <td>0.81460</td>\n",
       "      <td>0.18540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic1   Topic2\n",
       "Doc 1  0.00000  1.00000\n",
       "Doc 2  0.44050  0.55950\n",
       "Doc 3  1.00000  0.00000\n",
       "Doc 4  1.00000  0.00000\n",
       "Doc 5  1.00000  0.00000\n",
       "Doc 6  0.00000  1.00000\n",
       "Doc 7  0.00000  1.00000\n",
       "Doc 8  0.00000  1.00000\n",
       "Doc 9  0.94327  0.05673\n",
       "Doc10  0.81460  0.18540"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.round(pi_noback, 5), columns = ['Topic1', 'Topic2'],\\\n",
    "               index=['Doc 1', 'Doc 2', 'Doc 3', 'Doc 4', 'Doc 5', 'Doc 6', 'Doc 7', 'Doc 8', 'Doc 9', 'Doc10'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of PLSA,  \n",
    "** Topic1 ~ ( Doc 3, Doc 4, Doc 5, Doc 9, Doc 10 )**  \n",
    "** Topic2 ~ ( Doc 1, Doc 2, Doc 6, Doc 7, Doc 8 ) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Check P(w|z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Word Distribution Sorted by Topic 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.07235</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>network</th>\n",
       "      <td>0.04518</td>\n",
       "      <td>0.00456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>style</th>\n",
       "      <td>0.02713</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence</th>\n",
       "      <td>0.02713</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>0.02634</td>\n",
       "      <td>0.00739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generative</th>\n",
       "      <td>0.02412</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>0.02412</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adversarial</th>\n",
       "      <td>0.02110</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architecture</th>\n",
       "      <td>0.01809</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <td>0.01809</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adversary</th>\n",
       "      <td>0.01809</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>0.01750</td>\n",
       "      <td>0.01857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.01708</td>\n",
       "      <td>0.05741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generate</th>\n",
       "      <td>0.01507</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>using</th>\n",
       "      <td>0.01469</td>\n",
       "      <td>0.00935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robot</th>\n",
       "      <td>0.01332</td>\n",
       "      <td>0.03984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generating</th>\n",
       "      <td>0.01206</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medium</th>\n",
       "      <td>0.01206</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectrogram</th>\n",
       "      <td>0.01206</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <td>0.01123</td>\n",
       "      <td>0.03008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Topic1   Topic2\n",
       "model         0.07235  0.00000\n",
       "network       0.04518  0.00456\n",
       "style         0.02713  0.00000\n",
       "sequence      0.02713  0.00000\n",
       "neural        0.02634  0.00739\n",
       "generative    0.02412  0.00000\n",
       "text          0.02412  0.00000\n",
       "adversarial   0.02110  0.00000\n",
       "architecture  0.01809  0.00000\n",
       "language      0.01809  0.00000\n",
       "adversary     0.01809  0.00000\n",
       "training      0.01750  0.01857\n",
       "learning      0.01708  0.05741\n",
       "generate      0.01507  0.00000\n",
       "using         0.01469  0.00935\n",
       "robot         0.01332  0.03984\n",
       "generating    0.01206  0.00000\n",
       "medium        0.01206  0.00000\n",
       "spectrogram   0.01206  0.00000\n",
       "task          0.01123  0.03008"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nTopic Word Distribution Sorted by Topic 1')\n",
    "pd.DataFrame(np.round(topic_word_prob_noback.T, 5), columns = ['Topic1', 'Topic2'],\\\n",
    "            index = words_noback).sort_values('Topic1', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Word Distribution says 'model', 'network', 'sequence', 'generative', 'text', 'language' has high probability for topic1.  \n",
    "We can assume that this topic is related to NLP category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Word Distribution Sorted by Topic 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>0.01708</td>\n",
       "      <td>0.05741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reinforcement</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robot</th>\n",
       "      <td>0.01332</td>\n",
       "      <td>0.03984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>policy</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <td>0.01123</td>\n",
       "      <td>0.03008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>0.01750</td>\n",
       "      <td>0.01857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>based</th>\n",
       "      <td>0.00870</td>\n",
       "      <td>0.01838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>navigation</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learn</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agent</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>map</th>\n",
       "      <td>0.00301</td>\n",
       "      <td>0.01360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probabilistic</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sampling</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decomposition</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Topic1   Topic2\n",
       "learning       0.01708  0.05741\n",
       "reinforcement  0.00000  0.04532\n",
       "robot          0.01332  0.03984\n",
       "policy         0.00000  0.03626\n",
       "task           0.01123  0.03008\n",
       "long           0.00000  0.02493\n",
       "training       0.01750  0.01857\n",
       "based          0.00870  0.01838\n",
       "navigation     0.00000  0.01813\n",
       "learn          0.00000  0.01813\n",
       "deep           0.00000  0.01586\n",
       "term           0.00000  0.01360\n",
       "agent          0.00000  0.01360\n",
       "short          0.00000  0.01360\n",
       "map            0.00301  0.01360\n",
       "probabilistic  0.00000  0.01360\n",
       "real           0.00000  0.01360\n",
       "sampling       0.00000  0.01133\n",
       "decomposition  0.00000  0.01133\n",
       "road           0.00000  0.01133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nTopic Word Distribution Sorted by Topic 2')\n",
    "pd.DataFrame(np.round(topic_word_prob_noback.T, 5), columns = ['Topic1', 'Topic2'],\\\n",
    "            index = words_noback).sort_values('Topic2', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\nTopic Word Distribution Sorted by Topic 1')\n",
    "pd.DataFrame(np.round(topic_word_prob_noback.T, 5), columns = ['Topic1', 'Topic2'],\\\n",
    "            index = words_noback).sort_values('Topic1', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Word Distribution says 'learning', 'reinforcement', 'robot', 'policy', 'navigation' has high probability for topic2.  \n",
    "We can assume that this topic is related to Robotics category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "#  \n",
    "#  \n",
    "#  \n",
    "# APPENDIX : Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from collections import Counter, OrderedDict\n",
    "#from scipy.spatial.distance import pdist,squareform\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond word importance: using contextual decom...</td>\n",
       "      <td>The driving force behind the recent success of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generating Wikipedia by Summarizing Long Seque...</td>\n",
       "      <td>We show that generating English Wikipedia arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MaskGAN: Better Text Generation via Filling in...</td>\n",
       "      <td>Recurrent neural networks (RNNs) are a common ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Natural TTS Synthesis By Conditioning WaveNet ...</td>\n",
       "      <td>This paper describes Tacotron 2, a neural netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SHAPED: Shared-Private Encoder-Decoder for Tex...</td>\n",
       "      <td>Supervised training of abstractive language ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PRM-RL: Long-range Robotic Navigation Tasks by...</td>\n",
       "      <td>We present PRM-RL, a hierarchical method for l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Collective Robot Reinforcement Learning with D...</td>\n",
       "      <td>In principle, reinforcement learning and polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deep Reinforcement Learning for Robotic Manipu...</td>\n",
       "      <td>Reinforcement learning holds the promise of en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Learning to Manipulate Granular Media with a R...</td>\n",
       "      <td>In this paper, we examine the problem of robot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Supervision via Competition: Robot Adversaries...</td>\n",
       "      <td>There has been a recent paradigm shift in robo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Beyond word importance: using contextual decom...   \n",
       "1  Generating Wikipedia by Summarizing Long Seque...   \n",
       "2  MaskGAN: Better Text Generation via Filling in...   \n",
       "3  Natural TTS Synthesis By Conditioning WaveNet ...   \n",
       "4  SHAPED: Shared-Private Encoder-Decoder for Tex...   \n",
       "5  PRM-RL: Long-range Robotic Navigation Tasks by...   \n",
       "6  Collective Robot Reinforcement Learning with D...   \n",
       "7  Deep Reinforcement Learning for Robotic Manipu...   \n",
       "8  Learning to Manipulate Granular Media with a R...   \n",
       "9  Supervision via Competition: Robot Adversaries...   \n",
       "\n",
       "                                            contents  \n",
       "0  The driving force behind the recent success of...  \n",
       "1  We show that generating English Wikipedia arti...  \n",
       "2  Recurrent neural networks (RNNs) are a common ...  \n",
       "3  This paper describes Tacotron 2, a neural netw...  \n",
       "4  Supervised training of abstractive language ge...  \n",
       "5  We present PRM-RL, a hierarchical method for l...  \n",
       "6  In principle, reinforcement learning and polic...  \n",
       "7  Reinforcement learning holds the promise of en...  \n",
       "8  In this paper, we examine the problem of robot...  \n",
       "9  There has been a recent paradigm shift in robo...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'E:\\\\unist\\\\2-1\\\\'\n",
    "data = pd.read_csv(path+'abstract.csv', encoding='cp949')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "## 1. Do Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1.1. Normalization\n",
    "\n",
    "### Normalization 1, 2, 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1 = ['\\(CD\\)', 'CD', '\\(MOS\\)', '\\(RNNs\\)', 'LSTM', 'Yelp', 'SST', 'WaveNet', 'Q\\-function', 'GAN', 'RNN', 'Tacotron\\ 2', 'F0', 'FST', 'PRM\\-RL', 'PRMs',\\\n",
    "      'PRM', 'RL', '\\-\\ ', '\\-', 'TTS', 'MOS']\n",
    "l2 = ['', 'contenxtual decomposition', '', '', 'long short term memory', 'corporation', 'project', 'generative neural network',\\\n",
    "     'distribution', 'generative adversarial network', 'recurrent neural network', 'neural network model', 'accent',\\\n",
    "     'finite state transducer', 'probabilistic road map reinforcement learning', 'probabilistic road map','probabilistic road map', \\\n",
    "      'reinforcement learning', '', ' ', 'text to speach', 'mean opinion score']\n",
    "\n",
    "for a, b in zip(l1, l2):\n",
    "    data['contents'] = data['contents'].str.replace(a, ' '+b)\n",
    "    data['title'] = data['title'].str.replace(a, ' '+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beyond word importance: using contextual decom...</td>\n",
       "      <td>the driving force behind the recent success of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generating wikipedia by summarizing long seque...</td>\n",
       "      <td>we show that generating english wikipedia arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mask generative adversarial network: better te...</td>\n",
       "      <td>recurrent neural networks   are a common metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>natural  text to speach synthesis by condition...</td>\n",
       "      <td>this paper describes  neural network model, a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shaped: shared  private encoder  decoder for t...</td>\n",
       "      <td>supervised training of abstractive language ge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  beyond word importance: using contextual decom...   \n",
       "1  generating wikipedia by summarizing long seque...   \n",
       "2  mask generative adversarial network: better te...   \n",
       "3  natural  text to speach synthesis by condition...   \n",
       "4  shaped: shared  private encoder  decoder for t...   \n",
       "\n",
       "                                            contents  \n",
       "0  the driving force behind the recent success of...  \n",
       "1  we show that generating english wikipedia arti...  \n",
       "2  recurrent neural networks   are a common metho...  \n",
       "3  this paper describes  neural network model, a ...  \n",
       "4  supervised training of abstractive language ge...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'] = data['title'].str.lower()\n",
    "data['contents'] = data['contents'].str.lower()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    "### Tokenization\n",
    "Split sentences to words list by using non-word character\n",
    "\n",
    "1.1. Tokenization:  tokenize each document into tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [beyond, word, importance, :, using, contextua...\n",
       "1    [generating, wikipedia, by, summarizing, long,...\n",
       "2    [mask, generative, adversarial, network, :, be...\n",
       "3    [natural, text, to, speach, synthesis, by, con...\n",
       "4    [shaped, :, shared, private, encoder, decoder,...\n",
       "5    [probabilistic, road, map, reinforcement, lear...\n",
       "6    [collective, robot, reinforcement, learning, w...\n",
       "7    [deep, reinforcement, learning, for, robotic, ...\n",
       "8    [learning, to, manipulate, granular, media, wi...\n",
       "9    [supervision, via, competition, :, robot, adve...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'].apply(lambda x : word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[beyond, word, importance, :, using, contextua...</td>\n",
       "      <td>[the, driving, force, behind, the, recent, suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[generating, wikipedia, by, summarizing, long,...</td>\n",
       "      <td>[we, show, that, generating, english, wikipedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[mask, generative, adversarial, network, :, be...</td>\n",
       "      <td>[recurrent, neural, networks, are, a, common, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[natural, text, to, speach, synthesis, by, con...</td>\n",
       "      <td>[this, paper, describes, neural, network, mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[shaped, :, shared, private, encoder, decoder,...</td>\n",
       "      <td>[supervised, training, of, abstractive, langua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  [beyond, word, importance, :, using, contextua...   \n",
       "1   1  [generating, wikipedia, by, summarizing, long,...   \n",
       "2   2  [mask, generative, adversarial, network, :, be...   \n",
       "3   3  [natural, text, to, speach, synthesis, by, con...   \n",
       "4   4  [shaped, :, shared, private, encoder, decoder,...   \n",
       "\n",
       "                                            contents  \n",
       "0  [the, driving, force, behind, the, recent, suc...  \n",
       "1  [we, show, that, generating, english, wikipedi...  \n",
       "2  [recurrent, neural, networks, are, a, common, ...  \n",
       "3  [this, paper, describes, neural, network, mode...  \n",
       "4  [supervised, training, of, abstractive, langua...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = pd.DataFrame()\n",
    "doc['id'] = data.index\n",
    "doc['title'] = data['title'].apply(lambda x : word_tokenize(x))\n",
    "doc['contents'] = data['contents'].apply(lambda x : word_tokenize(x))\n",
    "doc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Extract only word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[beyond, word, importance, using, contextual, ...</td>\n",
       "      <td>[the, driving, force, behind, the, recent, suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[generating, wikipedia, by, summarizing, long,...</td>\n",
       "      <td>[we, show, that, generating, english, wikipedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[mask, generative, adversarial, network, bette...</td>\n",
       "      <td>[recurrent, neural, networks, are, a, common, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[natural, text, to, speach, synthesis, by, con...</td>\n",
       "      <td>[this, paper, describes, neural, network, mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[shaped, shared, private, encoder, decoder, fo...</td>\n",
       "      <td>[supervised, training, of, abstractive, langua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  [beyond, word, importance, using, contextual, ...   \n",
       "1   1  [generating, wikipedia, by, summarizing, long,...   \n",
       "2   2  [mask, generative, adversarial, network, bette...   \n",
       "3   3  [natural, text, to, speach, synthesis, by, con...   \n",
       "4   4  [shaped, shared, private, encoder, decoder, fo...   \n",
       "\n",
       "                                            contents  \n",
       "0  [the, driving, force, behind, the, recent, suc...  \n",
       "1  [we, show, that, generating, english, wikipedi...  \n",
       "2  [recurrent, neural, networks, are, a, common, ...  \n",
       "3  [this, paper, describes, neural, network, mode...  \n",
       "4  [supervised, training, of, abstractive, langua...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.filter(regex=r'[a-zA-Z]+')\n",
    "doc['title'] = doc['title'].apply(lambda x: [a for a in x if a not in string.punctuation])\n",
    "doc['contents'] = doc['contents'].apply(lambda x: [a for a in x if a not in string.punctuation])\n",
    "doc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[beyond, word, importance, using, contextual, ...</td>\n",
       "      <td>[the, driving, force, behind, the, recent, suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[generating, wikipedia, by, summarizing, long,...</td>\n",
       "      <td>[we, show, that, generating, english, wikipedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[mask, generative, adversarial, network, bette...</td>\n",
       "      <td>[recurrent, neural, network, are, a, common, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[natural, text, to, speach, synthesis, by, con...</td>\n",
       "      <td>[this, paper, describes, neural, network, mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[shaped, shared, private, encoder, decoder, fo...</td>\n",
       "      <td>[supervised, training, of, abstractive, langua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  [beyond, word, importance, using, contextual, ...   \n",
       "1   1  [generating, wikipedia, by, summarizing, long,...   \n",
       "2   2  [mask, generative, adversarial, network, bette...   \n",
       "3   3  [natural, text, to, speach, synthesis, by, con...   \n",
       "4   4  [shaped, shared, private, encoder, decoder, fo...   \n",
       "\n",
       "                                            contents  \n",
       "0  [the, driving, force, behind, the, recent, suc...  \n",
       "1  [we, show, that, generating, english, wikipedi...  \n",
       "2  [recurrent, neural, network, are, a, common, m...  \n",
       "3  [this, paper, describes, neural, network, mode...  \n",
       "4  [supervised, training, of, abstractive, langua...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['title'] = doc['title'].apply(lambda x: [wnl.lemmatize(a) for a in x])\n",
    "doc['contents'] = doc['contents'].apply(lambda x: [wnl.lemmatize(a) for a in x])\n",
    "doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [beyond, word, importance, using, contextual, ...\n",
       "1    [generating, wikipedia, by, summarizing, long,...\n",
       "2    [mask, generative, adversarial, network, bette...\n",
       "3    [natural, text, to, speach, synthesis, by, con...\n",
       "4    [shaped, shared, private, encoder, decoder, fo...\n",
       "5    [probabilistic, road, map, reinforcement, lear...\n",
       "6    [collective, robot, reinforcement, learning, w...\n",
       "7    [deep, reinforcement, learning, for, robotic, ...\n",
       "8    [learning, to, manipulate, granular, media, wi...\n",
       "9    [supervision, via, competition, robot, adversa...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc['all'] = doc['title'] + doc['contents']\n",
    "doc['all'] = doc['all'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[beyond, word, importance, using, contextual, ...</td>\n",
       "      <td>[the, driving, force, behind, the, recent, suc...</td>\n",
       "      <td>beyond word importance using contextual decomp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[generating, wikipedia, by, summarizing, long,...</td>\n",
       "      <td>[we, show, that, generating, english, wikipedi...</td>\n",
       "      <td>generating wikipedia by summarizing long seque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[mask, generative, adversarial, network, bette...</td>\n",
       "      <td>[recurrent, neural, network, are, a, common, m...</td>\n",
       "      <td>mask generative adversarial network better tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[natural, text, to, speach, synthesis, by, con...</td>\n",
       "      <td>[this, paper, describes, neural, network, mode...</td>\n",
       "      <td>natural text to speach synthesis by conditioni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[shaped, shared, private, encoder, decoder, fo...</td>\n",
       "      <td>[supervised, training, of, abstractive, langua...</td>\n",
       "      <td>shaped shared private encoder decoder for text...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  [beyond, word, importance, using, contextual, ...   \n",
       "1   1  [generating, wikipedia, by, summarizing, long,...   \n",
       "2   2  [mask, generative, adversarial, network, bette...   \n",
       "3   3  [natural, text, to, speach, synthesis, by, con...   \n",
       "4   4  [shaped, shared, private, encoder, decoder, fo...   \n",
       "\n",
       "                                            contents  \\\n",
       "0  [the, driving, force, behind, the, recent, suc...   \n",
       "1  [we, show, that, generating, english, wikipedi...   \n",
       "2  [recurrent, neural, network, are, a, common, m...   \n",
       "3  [this, paper, describes, neural, network, mode...   \n",
       "4  [supervised, training, of, abstractive, langua...   \n",
       "\n",
       "                                                 all  \n",
       "0  beyond word importance using contextual decomp...  \n",
       "1  generating wikipedia by summarizing long seque...  \n",
       "2  mask generative adversarial network better tex...  \n",
       "3  natural text to speach synthesis by conditioni...  \n",
       "4  shaped shared private encoder decoder for text...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc.to_csv(path+'abstract_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
